{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524184dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu129\n",
      "True\n",
      "12.9\n",
      "91002\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from fliqe import FLIQE\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(f\"Using device: {device}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "img_path = \"../data/video_frames_4/frame_0000.jpg\"\n",
    "img = cv2.imread(img_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a105e59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\k_chi\\vscodeProjects\\FLIr Image Quality Estimator (FLIQE)\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\k_chi\\vscodeProjects\\FLIr Image Quality Estimator (FLIQE)\\.venv\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIQE score (lower better): 3.859058813628418\n",
      "PIQE score (lower better): 29.576688766479492\n",
      "BRISQUE score (lower better): 10.10565185546875\n",
      "ARNIQA score (higher better): 0.5651703476905823\n",
      "Loading pretrained model PAQ2PIQ from C:\\Users\\k_chi\\.cache\\torch\\hub\\pyiqa\\P2P_RoIPoolModel-fit.10.bs.120-ca69882e.pth\n",
      "PAQ2PIQ score (higher better): 68.78011322021484\n",
      "Loading pretrained model MUSIQ from C:\\Users\\k_chi\\.cache\\torch\\hub\\pyiqa\\musiq_ava_ckpt-e8d3f067.pth\n",
      "MUSIQ score (higher better): 4.8943681716918945\n",
      "Loading pretrained model SCNN from C:\\Users\\k_chi\\.cache\\torch\\hub\\pyiqa\\DBCNN_scnn-7ea73d75.pth\n",
      "Loading pretrained model DBCNN from C:\\Users\\k_chi\\.cache\\torch\\hub\\pyiqa\\DBCNN_KonIQ10k-2de81c0a.pth\n",
      "DBCNN score (higher better): 0.44894492626190186\n",
      "CLIPIQA score (higher better): 0.38447484374046326\n",
      "FLIQE score (higher better): 0.9513196349143982\n",
      "FLIQE score from tensor (higher better): -2.9725747108459473\n"
     ]
    }
   ],
   "source": [
    "from pyiqa.archs.niqe_arch import NIQE\n",
    "from pyiqa.archs.piqe_arch import PIQE\n",
    "from pyiqa.archs.brisque_arch import BRISQUE\n",
    "from pyiqa.archs.arniqa_arch import ARNIQA\n",
    "from pyiqa.archs.paq2piq_arch import PAQ2PIQ\n",
    "from pyiqa.archs.musiq_arch import MUSIQ\n",
    "from pyiqa.archs.dbcnn_arch import DBCNN\n",
    "from pyiqa.archs.clipiqa_arch import CLIPIQA\n",
    "from pyiqa.archs.maniqa_arch import MANIQA\n",
    "from torchvision import transforms\n",
    "\n",
    "img_tensor = transforms.ToTensor()(img).unsqueeze(0)\n",
    "\n",
    "niqe = NIQE()\n",
    "niqe_score = niqe(img_tensor)\n",
    "print(\"NIQE score (lower better):\", float(niqe_score))\n",
    "\n",
    "piqe = PIQE()\n",
    "piqe_score = piqe(img_tensor)\n",
    "print(\"PIQE score (lower better):\", float(piqe_score))\n",
    "\n",
    "brisque = BRISQUE()\n",
    "brisque_score = brisque(img_tensor)\n",
    "print(\"BRISQUE score (lower better):\", float(brisque_score.detach()))\n",
    "\n",
    "arniqa = ARNIQA()\n",
    "img_tensor = img_tensor.to(device)\n",
    "arniqa = arniqa.to(device)\n",
    "arniqa_score = arniqa(img_tensor)\n",
    "print(\"ARNIQA score (higher better):\", float(arniqa_score.detach()))\n",
    "\n",
    "paq2piq = PAQ2PIQ()\n",
    "paq2piq.eval()  # Ensure model is in evaluation mode\n",
    "img_tensor = img_tensor.to(device)\n",
    "paq2piq = paq2piq.to(device)\n",
    "paq2piq_score = paq2piq(img_tensor)\n",
    "print(\"PAQ2PIQ score (higher better):\", float(paq2piq_score.detach()))\n",
    "\n",
    "musiq = MUSIQ()\n",
    "musiq.eval()  # Ensure model is in evaluation mode\n",
    "img_tensor = img_tensor.to(device)\n",
    "musiq = musiq.to(device)\n",
    "musiq_score = musiq(img_tensor)\n",
    "print(\"MUSIQ score (higher better):\", float(musiq_score.detach()))\n",
    "\n",
    "dbcnn = DBCNN()\n",
    "dbcnn.eval()  # Ensure model is in evaluation mode\n",
    "img_tensor = img_tensor.to(device)\n",
    "dbcnn = dbcnn.to(device)\n",
    "dbcnn_score = dbcnn(img_tensor)\n",
    "print(\"DBCNN score (higher better):\", float(dbcnn_score.detach()))\n",
    "\n",
    "clipiqa = CLIPIQA()\n",
    "clipiqa.eval()  # Ensure model is in evaluation mode\n",
    "img_tensor = img_tensor.to(device)\n",
    "clipiqa = clipiqa.to(device)\n",
    "clipiqa_score = clipiqa(img_tensor)\n",
    "print(\"CLIPIQA score (higher better):\", float(clipiqa_score.detach()))\n",
    "\n",
    "# maniqa = MANIQA()\n",
    "# maniqa.eval()  # Ensure model is in evaluation mode\n",
    "# img_tensor = img_tensor.to(device)\n",
    "# maniqa = maniqa.to(device)\n",
    "# maniqa_score = maniqa(img_tensor)\n",
    "# print(\"MANIQA score (higher better):\", float(maniqa_score.detach()))\n",
    "\n",
    "fliqe = FLIQE(quality_model_path=\"../models/encoder_with_binary_head.pth\")\n",
    "FLIQE_score = fliqe.estimate_image_quality(img)\n",
    "print(\"FLIQE score (higher better):\", FLIQE_score)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)), # 224 or 384\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "img_tensor = transform(img).unsqueeze(0)\n",
    "img_tensor = img_tensor.to(device)\n",
    "FLIQE_score_tensor = fliqe.quality_model(img_tensor).item()\n",
    "print(\"FLIQE score from tensor (higher better):\", FLIQE_score_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd0dab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset length: 10742\n",
      "Eval Dataset length: 1144\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from fliqe.datasets import ImageDataset\n",
    "from fliqe.distortions import *\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "    \n",
    "distortions = [Clean(), LensBlur(), MotionBlur(), GaussianNoise(), Overexposure(), Underexposure(), Compression(), Ghosting(), Aliasing()]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)), # 224 or 384\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# # All folders with images\n",
    "# # all_folders = [f\"../data/video_frames_{i}\" for i in range(1, 13)]\n",
    "# # all_folders = [\"../data/FLIR_ADAS_v2/video_thermal_test/data\"]\n",
    "# all_folders = [\"../data/FLIR_ADAS_v2/images_thermal_train/data\"]\n",
    "# # Gather all image paths\n",
    "# all_image_paths = []\n",
    "# for folder in all_folders:\n",
    "#     if not os.path.isdir(folder):\n",
    "#         print(f\"Warning: folder {folder} not found, skipping.\")\n",
    "#         continue\n",
    "#     all_image_paths.extend(\n",
    "#         [os.path.join(folder, fname)\n",
    "#          for fname in os.listdir(folder)\n",
    "#          if fname.lower().endswith(('.jpg', '.png'))]\n",
    "#     )\n",
    "# print(f\"Total images found: {len(all_image_paths)}\")\n",
    "# # Keep randomly 10000 images\n",
    "# # all_image_paths = random.sample(all_image_paths, min(10000, len(all_image_paths)))\n",
    "# # print(f\"Total random images: {len(all_image_paths)}\")\n",
    "# # Shuffle and split\n",
    "# # random.shuffle(all_image_paths)\n",
    "# split_ratio = 0.8  # 80% train, 20% eval\n",
    "# split_idx = int(len(all_image_paths) * split_ratio)\n",
    "# train_paths = all_image_paths[:split_idx]\n",
    "# eval_paths = all_image_paths[split_idx:]\n",
    "# # Create datasets\n",
    "# train_dataset = ImageDataset(train_paths, distortions=distortions, transform=transform, binary_labels=True)\n",
    "# eval_dataset = ImageDataset(eval_paths, distortions=distortions, transform=transform, binary_labels=True)\n",
    "# # Create dataloaders\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "# eval_dataloader = DataLoader(eval_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "# print(f\"Train Dataset length: {len(train_dataset)}\")\n",
    "# print(f\"Eval Dataset length: {len(eval_dataset)}\")\n",
    "\n",
    "image_folder = \"../data/FLIR_ADAS_v2/images_thermal_train/data\"\n",
    "image_paths = [os.path.join(image_folder, fname) for fname in os.listdir(image_folder) if fname.endswith(('.jpg', '.png'))]\n",
    "train_dataset = ImageDataset(image_paths, distortions=distortions, transform=transform, binary_labels=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "print(f\"Train Dataset length: {len(train_dataset)}\")\n",
    "# image_folder = \"../data/FLIR_ADAS_v2/video_thermal_test/data\"\n",
    "image_folder = \"../data/FLIR_ADAS_v2/images_thermal_val/data\"\n",
    "image_paths = [os.path.join(image_folder, fname) for fname in os.listdir(image_folder) if fname.endswith(('.jpg', '.png'))]\n",
    "eval_dataset = ImageDataset(image_paths, distortions=distortions, transform=transform, binary_labels=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "print(f\"Eval Dataset length: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59529261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model PAQ2PIQ from C:\\Users\\k_chi\\.cache\\torch\\hub\\pyiqa\\P2P_RoIPoolModel-fit.10.bs.120-ca69882e.pth\n",
      "Loading pretrained model MUSIQ from C:\\Users\\k_chi\\.cache\\torch\\hub\\pyiqa\\musiq_ava_ckpt-e8d3f067.pth\n",
      "Loading pretrained model SCNN from C:\\Users\\k_chi\\.cache\\torch\\hub\\pyiqa\\DBCNN_scnn-7ea73d75.pth\n",
      "Loading pretrained model DBCNN from C:\\Users\\k_chi\\.cache\\torch\\hub\\pyiqa\\DBCNN_KonIQ10k-2de81c0a.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLIQE - Train: 10742 scores, Eval: 1144 scores\n",
      "NIQE - Train: 10742 scores, Eval: 1144 scores\n",
      "PIQE - Train: 10742 scores, Eval: 1144 scores\n",
      "BRISQUE - Train: 10742 scores, Eval: 1144 scores\n",
      "ARNIQA - Train: 10742 scores, Eval: 1144 scores\n",
      "PAQ2PIQ - Train: 10742 scores, Eval: 1144 scores\n",
      "MUSIQ - Train: 10742 scores, Eval: 1144 scores\n",
      "DBCNN - Train: 10742 scores, Eval: 1144 scores\n",
      "CLIPIQA - Train: 10742 scores, Eval: 1144 scores\n",
      "\n",
      "================================================================================\n",
      "ALGORITHM COMPARISON RESULTS\n",
      "================================================================================\n",
      "\n",
      "FLIQE Results:\n",
      "--------------------------------------------------\n",
      "Accuracy : 0.859\n",
      "Precision: 0.898\n",
      "Recall   : 0.773\n",
      "F1-score : 0.831\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.835     0.929     0.879       632\n",
      "         1.0      0.898     0.773     0.831       512\n",
      "\n",
      "    accuracy                          0.859      1144\n",
      "   macro avg      0.866     0.851     0.855      1144\n",
      "weighted avg      0.863     0.859     0.858      1144\n",
      "\n",
      "\n",
      "NIQE Results:\n",
      "--------------------------------------------------\n",
      "Accuracy : 0.789\n",
      "Precision: 0.814\n",
      "Recall   : 0.686\n",
      "F1-score : 0.744\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.774     0.873     0.821       632\n",
      "         1.0      0.814     0.686     0.744       512\n",
      "\n",
      "    accuracy                          0.789      1144\n",
      "   macro avg      0.794     0.779     0.783      1144\n",
      "weighted avg      0.792     0.789     0.787      1144\n",
      "\n",
      "\n",
      "PIQE Results:\n",
      "--------------------------------------------------\n",
      "Accuracy : 0.792\n",
      "Precision: 0.883\n",
      "Recall   : 0.617\n",
      "F1-score : 0.726\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.751     0.934     0.832       632\n",
      "         1.0      0.883     0.617     0.726       512\n",
      "\n",
      "    accuracy                          0.792      1144\n",
      "   macro avg      0.817     0.775     0.779      1144\n",
      "weighted avg      0.810     0.792     0.785      1144\n",
      "\n",
      "\n",
      "BRISQUE Results:\n",
      "--------------------------------------------------\n",
      "Accuracy : 0.821\n",
      "Precision: 0.887\n",
      "Recall   : 0.688\n",
      "F1-score : 0.774\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.786     0.929     0.851       632\n",
      "         1.0      0.887     0.688     0.774       512\n",
      "\n",
      "    accuracy                          0.821      1144\n",
      "   macro avg      0.836     0.808     0.813      1144\n",
      "weighted avg      0.831     0.821     0.817      1144\n",
      "\n",
      "\n",
      "ARNIQA Results:\n",
      "--------------------------------------------------\n",
      "Accuracy : 0.810\n",
      "Precision: 0.813\n",
      "Recall   : 0.748\n",
      "F1-score : 0.779\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.808     0.861     0.834       632\n",
      "         1.0      0.813     0.748     0.779       512\n",
      "\n",
      "    accuracy                          0.810      1144\n",
      "   macro avg      0.811     0.804     0.806      1144\n",
      "weighted avg      0.810     0.810     0.809      1144\n",
      "\n",
      "\n",
      "PAQ2PIQ Results:\n",
      "--------------------------------------------------\n",
      "Accuracy : 0.743\n",
      "Precision: 0.752\n",
      "Recall   : 0.635\n",
      "F1-score : 0.689\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.737     0.831     0.781       632\n",
      "         1.0      0.752     0.635     0.689       512\n",
      "\n",
      "    accuracy                          0.743      1144\n",
      "   macro avg      0.745     0.733     0.735      1144\n",
      "weighted avg      0.744     0.743     0.740      1144\n",
      "\n",
      "\n",
      "MUSIQ Results:\n",
      "--------------------------------------------------\n",
      "Accuracy : 0.729\n",
      "Precision: 0.713\n",
      "Recall   : 0.660\n",
      "F1-score : 0.686\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.740     0.785     0.762       632\n",
      "         1.0      0.713     0.660     0.686       512\n",
      "\n",
      "    accuracy                          0.729      1144\n",
      "   macro avg      0.727     0.722     0.724      1144\n",
      "weighted avg      0.728     0.729     0.728      1144\n",
      "\n",
      "\n",
      "DBCNN Results:\n",
      "--------------------------------------------------\n",
      "Accuracy : 0.736\n",
      "Precision: 0.717\n",
      "Recall   : 0.678\n",
      "F1-score : 0.697\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.750     0.783     0.766       632\n",
      "         1.0      0.717     0.678     0.697       512\n",
      "\n",
      "    accuracy                          0.736      1144\n",
      "   macro avg      0.733     0.730     0.732      1144\n",
      "weighted avg      0.735     0.736     0.735      1144\n",
      "\n",
      "\n",
      "CLIPIQA Results:\n",
      "--------------------------------------------------\n",
      "Accuracy : 0.716\n",
      "Precision: 0.700\n",
      "Recall   : 0.639\n",
      "F1-score : 0.668\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.727     0.778     0.752       632\n",
      "         1.0      0.700     0.639     0.668       512\n",
      "\n",
      "    accuracy                          0.716      1144\n",
      "   macro avg      0.713     0.709     0.710      1144\n",
      "weighted avg      0.715     0.716     0.714      1144\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SUMMARY COMPARISON TABLE\n",
      "================================================================================\n",
      "Algorithm    Accuracy   Precision    Recall     F1-Score  \n",
      "------------------------------------------------------------\n",
      "FLIQE        0.859      0.898        0.773      0.831     \n",
      "NIQE         0.789      0.814        0.686      0.744     \n",
      "PIQE         0.792      0.883        0.617      0.726     \n",
      "BRISQUE      0.821      0.887        0.688      0.774     \n",
      "ARNIQA       0.810      0.813        0.748      0.779     \n",
      "PAQ2PIQ      0.743      0.752        0.635      0.689     \n",
      "MUSIQ        0.729      0.713        0.660      0.686     \n",
      "DBCNN        0.736      0.717        0.678      0.697     \n",
      "CLIPIQA      0.716      0.700        0.639      0.668     \n",
      "\n",
      "Best performing algorithm (by F1-score): FLIQE (F1: 0.831)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Initialize all comparison algorithms\n",
    "comparison_algos = {\n",
    "    \"FLIQE\": {\"train_scores\": [], \"train_labels\": [], \"eval_scores\": [], \"eval_labels\": [], \"model\": fliqe.quality_model},\n",
    "    \"NIQE\": {\"train_scores\": [], \"train_labels\": [], \"eval_scores\": [], \"eval_labels\": [], \"model\": NIQE()},\n",
    "    \"PIQE\": {\"train_scores\": [], \"train_labels\": [], \"eval_scores\": [], \"eval_labels\": [], \"model\": PIQE()},\n",
    "    \"BRISQUE\": {\"train_scores\": [], \"train_labels\": [], \"eval_scores\": [], \"eval_labels\": [], \"model\": BRISQUE()},\n",
    "    \"ARNIQA\": {\"train_scores\": [], \"train_labels\": [], \"eval_scores\": [], \"eval_labels\": [], \"model\": ARNIQA().to(device)},\n",
    "    \"PAQ2PIQ\": {\"train_scores\": [], \"train_labels\": [], \"eval_scores\": [], \"eval_labels\": [], \"model\": PAQ2PIQ().eval().to(device)},\n",
    "    \"MUSIQ\": {\"train_scores\": [], \"train_labels\": [], \"eval_scores\": [], \"eval_labels\": [], \"model\": MUSIQ().eval().to(device)},\n",
    "    \"DBCNN\": {\"train_scores\": [], \"train_labels\": [], \"eval_scores\": [], \"eval_labels\": [], \"model\": DBCNN().eval().to(device)},\n",
    "    \"CLIPIQA\": {\"train_scores\": [], \"train_labels\": [], \"eval_scores\": [], \"eval_labels\": [], \"model\": CLIPIQA().eval().to(device)}\n",
    "}\n",
    "\n",
    "def compute_scores_for_all_algos(dataloader, algo_dict, data_type=\"train\"):\n",
    "    for imgs, labels in tqdm(dataloader, desc=f\"Computing {data_type} Scores\", leave=False):\n",
    "        imgs = imgs.to(device)\n",
    "        binary_labels = torch.tensor(\n",
    "            [0.0 if l == 'Clean' else 1.0 for l in labels],\n",
    "            dtype=torch.float32, device=device\n",
    "        ).unsqueeze(1)\n",
    "        \n",
    "        for algo_name, algo_data in algo_dict.items():\n",
    "            model = algo_data[\"model\"]\n",
    "            scores = model(imgs)\n",
    "            \n",
    "            # Store scores and labels for each algorithm\n",
    "            algo_data[f\"{data_type}_scores\"].extend(scores.detach().cpu().numpy().flatten())\n",
    "            algo_data[f\"{data_type}_labels\"].extend(binary_labels.cpu().numpy().flatten())\n",
    "\n",
    "# Compute scores for all algorithms on both datasets\n",
    "compute_scores_for_all_algos(train_dataloader, comparison_algos, \"train\")\n",
    "compute_scores_for_all_algos(eval_dataloader, comparison_algos, \"eval\")\n",
    "\n",
    "# Print dataset sizes for verification\n",
    "for algo_name in comparison_algos.keys():\n",
    "    train_len = len(comparison_algos[algo_name][\"train_scores\"])\n",
    "    eval_len = len(comparison_algos[algo_name][\"eval_scores\"])\n",
    "    print(f\"{algo_name} - Train: {train_len} scores, Eval: {eval_len} scores\")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALGORITHM COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Evaluate each algorithm\n",
    "results_summary = {}\n",
    "\n",
    "for algo_name, algo_data in comparison_algos.items():\n",
    "    print(f\"\\n{algo_name} Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    train_scores = np.array(algo_data[\"train_scores\"]).reshape(-1, 1)\n",
    "    train_labels = np.array(algo_data[\"train_labels\"])\n",
    "    \n",
    "    eval_scores = np.array(algo_data[\"eval_scores\"]).reshape(-1, 1)\n",
    "    eval_labels = np.array(algo_data[\"eval_labels\"])\n",
    "    \n",
    "    # Step 1: Fit logistic regression for Platt scaling\n",
    "    platt_model = LogisticRegression()\n",
    "    platt_model.fit(train_scores, train_labels)\n",
    "    \n",
    "    # Step 2: Apply scaling to get calibrated probabilities\n",
    "    calibrated_probs = platt_model.predict_proba(eval_scores)[:, 1]\n",
    "    \n",
    "    threshold = 0.5\n",
    "    eval_preds = (calibrated_probs >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(eval_labels, eval_preds)\n",
    "    precision = precision_score(eval_labels, eval_preds)\n",
    "    recall = recall_score(eval_labels, eval_preds)\n",
    "    f1 = f1_score(eval_labels, eval_preds)\n",
    "    \n",
    "    # Store results for comparison\n",
    "    results_summary[algo_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"Accuracy : {accuracy:.3f}\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall   : {recall:.3f}\")\n",
    "    print(f\"F1-score : {f1:.3f}\")\n",
    "    \n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(eval_labels, eval_preds, digits=3))\n",
    "\n",
    "# Summary comparison table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Algorithm':<12} {'Accuracy':<10} {'Precision':<12} {'Recall':<10} {'F1-Score':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for algo_name, metrics in results_summary.items():\n",
    "    print(f\"{algo_name:<12} {metrics['accuracy']:<10.3f} {metrics['precision']:<12.3f} {metrics['recall']:<10.3f} {metrics['f1']:<10.3f}\")\n",
    "\n",
    "# Find best performing algorithm\n",
    "best_f1_algo = max(results_summary.keys(), key=lambda x: results_summary[x]['f1'])\n",
    "print(f\"\\nBest performing algorithm (by F1-score): {best_f1_algo} (F1: {results_summary[best_f1_algo]['f1']:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
