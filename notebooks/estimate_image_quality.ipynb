{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed21e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img_path = \"../data/video_frames_4/frame_0000.jpg\"\n",
    "img = cv2.imread(img_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "524184dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLIQE score: 0.9513196349143982\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from fliqe import FLIQE\n",
    "\n",
    "fliqe = FLIQE(quality_model_path=\"../models/encoder_with_binary_head.pth\")\n",
    "FLIQE_score = fliqe.estimate_image_quality(img)\n",
    "print(\"FLIQE score:\", FLIQE_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c8506b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\k_chi\\vscodeProjects\\FLIr Image Quality Estimator (FLIQE)\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyiqa\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# create metric with default setting\n",
    "iqa_metric = pyiqa.create_metric('arniqa', device=device)\n",
    "\n",
    "# check if lower better or higher better\n",
    "print(iqa_metric.lower_better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a105e59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIQE score (lower better): 3.859058813628418\n",
      "PIQE score (lower better): 29.576688766479492\n",
      "BRISQUE score (lower better): 10.10565185546875\n",
      "ARNIQA score (higher better): 0.5652608871459961\n",
      "Loading pretrained model PAQ2PIQ from C:\\Users\\k_chi\\.cache\\torch\\hub\\pyiqa\\P2P_RoIPoolModel-fit.10.bs.120-ca69882e.pth\n",
      "PAQ2PIQ score (higher better): 68.7802963256836\n"
     ]
    }
   ],
   "source": [
    "from pyiqa.archs.niqe_arch import NIQE\n",
    "from pyiqa.archs.piqe_arch import PIQE\n",
    "from pyiqa.archs.brisque_arch import BRISQUE\n",
    "from pyiqa.archs.arniqa_arch import ARNIQA\n",
    "from pyiqa.archs.paq2piq_arch import PAQ2PIQ\n",
    "from torchvision import transforms\n",
    "\n",
    "img_tensor = transforms.ToTensor()(img).unsqueeze(0)\n",
    "\n",
    "niqe = NIQE()\n",
    "niqe_score = niqe(img_tensor)\n",
    "print(\"NIQE score (lower better):\", float(niqe_score))\n",
    "\n",
    "piqe = PIQE()\n",
    "piqe_score = piqe(img_tensor)\n",
    "print(\"PIQE score (lower better):\", float(piqe_score))\n",
    "\n",
    "brisque = BRISQUE()\n",
    "brisque_score = brisque(img_tensor)\n",
    "print(\"BRISQUE score (lower better):\", float(brisque_score.detach()))\n",
    "\n",
    "arniqa = ARNIQA()\n",
    "arniqa_score = arniqa(img_tensor)\n",
    "print(\"ARNIQA score (higher better):\", float(arniqa_score.detach()))\n",
    "\n",
    "paq2piq = PAQ2PIQ()\n",
    "paq2piq.eval()  # Ensure model is in evaluation mode\n",
    "paq2piq_score = paq2piq(img_tensor)\n",
    "print(\"PAQ2PIQ score (higher better):\", float(paq2piq_score.detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0dab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu129\n",
      "True\n",
      "12.9\n",
      "91002\n",
      "Using device: cuda\n",
      "Total images found: 80670\n",
      "Train Dataset length: 64536\n",
      "Eval Dataset length: 16134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     76\u001b[39m         label_list.extend(binary_labels.cpu().numpy().flatten())\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Compute for training set\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[43mcompute_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Compute for evaluation set\u001b[39;00m\n\u001b[32m     83\u001b[39m compute_scores(eval_dataloader, eval_scores, eval_labels)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mcompute_scores\u001b[39m\u001b[34m(dataloader, score_list, label_list)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# NIQE expects image tensors in a specific format — adjust if needed\u001b[39;00m\n\u001b[32m     74\u001b[39m scores = niqe(imgs)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[43mscore_list\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m label_list.extend(binary_labels.cpu().numpy().flatten())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\k_chi\\vscodeProjects\\FLIr Image Quality Estimator (FLIQE)\\.venv\\Lib\\site-packages\\torch\\_tensor.py:1186\u001b[39m, in \u001b[36mTensor.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1177\u001b[39m     \u001b[38;5;66;03m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001b[39;00m\n\u001b[32m   1178\u001b[39m     \u001b[38;5;66;03m# generator and don't eagerly perform all the indexes.  This could\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1183\u001b[39m     \u001b[38;5;66;03m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001b[39;00m\n\u001b[32m   1184\u001b[39m     \u001b[38;5;66;03m# See gh-54457\u001b[39;00m\n\u001b[32m   1185\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dim() == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1186\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33miteration over a 0-d tensor\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1187\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._C._get_tracing_state():\n\u001b[32m   1188\u001b[39m         warnings.warn(\n\u001b[32m   1189\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1190\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPassing a tensor of different shape won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt change the number of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1194\u001b[39m             stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1195\u001b[39m         )\n",
      "\u001b[31mTypeError\u001b[39m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from fliqe.datasets import ImageDataset\n",
    "from fliqe.distortions import *\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(f\"Using device: {device}\")\n",
    "    \n",
    "distortions = [Clean(), LensBlur(), MotionBlur(), GaussianNoise(), Overexposure(), Underexposure(), Compression(), Ghosting(), Aliasing()]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)), # 224 or 384\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# All folders with images\n",
    "all_folders = [f\"../data/video_frames_{i}\" for i in range(1, 13)]\n",
    "# Gather all image paths\n",
    "all_image_paths = []\n",
    "for folder in all_folders:\n",
    "    if not os.path.isdir(folder):\n",
    "        print(f\"Warning: folder {folder} not found, skipping.\")\n",
    "        continue\n",
    "    all_image_paths.extend(\n",
    "        [os.path.join(folder, fname)\n",
    "         for fname in os.listdir(folder)\n",
    "         if fname.lower().endswith(('.jpg', '.png'))]\n",
    "    )\n",
    "print(f\"Total images found: {len(all_image_paths)}\")\n",
    "\n",
    "# Shuffle and split\n",
    "random.shuffle(all_image_paths)\n",
    "split_ratio = 0.8  # 80% train, 20% eval\n",
    "split_idx = int(len(all_image_paths) * split_ratio)\n",
    "train_paths = all_image_paths[:split_idx]\n",
    "eval_paths = all_image_paths[split_idx:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ImageDataset(train_paths, distortions=distortions, transform=transform, binary_labels=True)\n",
    "eval_dataset = ImageDataset(eval_paths, distortions=distortions, transform=transform, binary_labels=True)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True,\n",
    "                              num_workers=16, pin_memory=True, persistent_workers=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=1, shuffle=False,\n",
    "                             num_workers=16, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "print(f\"Train Dataset length: {len(train_dataset)}\")\n",
    "print(f\"Eval Dataset length: {len(eval_dataset)}\")\n",
    "\n",
    "train_scores, train_labels = [], []\n",
    "eval_scores, eval_labels = [], []\n",
    "\n",
    "def compute_scores(dataloader, score_list, label_list):\n",
    "    for imgs, labels in tqdm(dataloader, desc=\"Computing Scores\", leave=False):\n",
    "        imgs = imgs.to(device)\n",
    "        binary_labels = torch.tensor(\n",
    "            [0.0 if l == 'Clean' else 1.0 for l in labels],\n",
    "            dtype=torch.float32, device=device\n",
    "        ).unsqueeze(1)\n",
    "        # NIQE expects image tensors in a specific format — adjust if needed\n",
    "        scores = niqe(imgs)\n",
    "        score_list.extend(scores)\n",
    "        label_list.extend(binary_labels.cpu().numpy().flatten())\n",
    "\n",
    "\n",
    "# Compute for training set\n",
    "compute_scores(train_dataloader, train_scores, train_labels)\n",
    "\n",
    "# Compute for evaluation set\n",
    "compute_scores(eval_dataloader, eval_scores, eval_labels)\n",
    "\n",
    "print(f\"Train: {len(train_scores)} scores, Eval: {len(eval_scores)} scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5804f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# convert lists to arrays\n",
    "X_train = np.array(train_scores).reshape(-1, 1)\n",
    "y_train = np.array(train_labels)\n",
    "\n",
    "X_eval = np.array(eval_scores).reshape(-1, 1)\n",
    "y_eval = np.array(eval_labels)\n",
    "\n",
    "# Fit logistic regression (Platt scaling)\n",
    "platt = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "platt.fit(X_train, y_train)\n",
    "\n",
    "# Calibrated probabilities\n",
    "train_probs = platt.predict_proba(X_train)[:, 1]\n",
    "eval_probs  = platt.predict_proba(X_eval)[:, 1]\n",
    "\n",
    "# Default threshold 0.5\n",
    "eval_preds_05 = (eval_probs >= 0.5).astype(int)\n",
    "print(\"Eval F1 (threshold 0.5):\", f1_score(y_eval, eval_preds_05))\n",
    "print(classification_report(y_eval, eval_preds_05))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
